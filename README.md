# LexArena

**A live benchmark for evaluating AI models on SEC enforcement case predictions.**

LexArena tests whether AI systems can correctly predict real SEC enforcement outcomes before they happen. Models are given complaint text and must forecast resolution type, monetary penalties, and remedial measures.

## ğŸ¯ Overview

Legal prediction is a challenging domain requiring both factual extraction and probabilistic reasoning. LexArena provides:

- **Real cases** â€” 500+ resolved SEC enforcement actions with verified outcomes
- **Blind evaluation** â€” Models see only the original complaint, never the outcome
- **Standardized metrics** â€” 6 prediction targets with consistent scoring
- **Open leaderboard** â€” Compare models on the same benchmark

## ğŸ“Š Current Results

| Model | Overall | Resolution | Monetary | Injunction | Officer Bar |
|-------|---------|------------|----------|------------|-------------|
| GPT-4o | **64.9%** | 38.6% | 53.0% | 78.8% | 89.2% |
| Claude 3.5 | â€” | â€” | â€” | â€” | â€” |
| Gemini 2.0 | â€” | â€” | â€” | â€” | â€” |

## ğŸ”® Prediction Metrics

Models predict 6 outcome metrics for each case:

| Metric | Description | Scoring |
|--------|-------------|---------|
| **Resolution Type** | Settled vs. litigated | Exact match |
| **Disgorgement** | Amount of ill-gotten gains returned | Â±10% tolerance |
| **Civil Penalty** | Fine amount imposed | Â±10% tolerance |
| **Prejudgment Interest** | Interest on disgorgement | Â±10% tolerance |
| **Injunction** | Court order preventing future violations | Exact match |
| **Officer/Director Bar** | Ban from serving as company officer | Exact match |

## ğŸ—ï¸ How It Works

```
SEC Complaint PDF â†’ Text Extraction â†’ LLM Prediction â†’ Compare to Ground Truth â†’ Score
```

1. **Data Collection** â€” SEC litigation releases are scraped with complaint PDFs
2. **Synopsis Generation** â€” GPT-4o generates plain-English case summaries
3. **Prediction** â€” Models receive synopsis and predict 6 outcomes
4. **Scoring** â€” Predictions compared against actual SEC release outcomes

## ğŸš€ Getting Started

### Prerequisites

```bash
pip install -r requirements.txt
```

Required environment variables:
```bash
export OPENAI_API_KEY=your_key_here
```

### Run Evaluation

```bash
# Evaluate GPT-4o on all cases
python run_evaluation.py --evaluate --provider openai --model gpt-4o

# Limit to first 50 cases
python run_evaluation.py --evaluate --max-eval-cases 50 --save-results
```

### Generate Synopses

```bash
# Generate case summaries for all cases
python generate_synopses.py

# Limit to specific number
python generate_synopses.py --limit 100
```

### Update Website

```bash
python generate_viewer.py
```

## ğŸ“ Project Structure

```
lexarena/
â”œâ”€â”€ index.html              # Landing page with leaderboard
â”œâ”€â”€ cases.html              # All cases viewer with search
â”œâ”€â”€ run_evaluation.py       # Main evaluation script
â”œâ”€â”€ generate_synopses.py    # GPT-4o synopsis generation
â”œâ”€â”€ generate_viewer.py      # HTML generation from results
â”œâ”€â”€ litigation-cases.json   # Raw SEC case data
â”œâ”€â”€ data/
â”‚   â””â”€â”€ processed/
â”‚       â”œâ”€â”€ evaluation_results_openai.json
â”‚       â””â”€â”€ evaluation_dataset.json
â””â”€â”€ src/
    â”œâ”€â”€ evaluation/
    â”‚   â”œâ”€â”€ llm_prompt_formatter.py
    â”‚   â”œâ”€â”€ llm_runner.py
    â”‚   â””â”€â”€ score_calculator.py
    â””â”€â”€ preprocessing/
        â”œâ”€â”€ dataset_builder.py
        â”œâ”€â”€ ground_truth_extractor.py
        â””â”€â”€ synopsis_generator.py
```

## ğŸ¤ Contributing

We welcome contributions! To add a new model:

1. Add provider support in `src/evaluation/llm_runner.py`
2. Run evaluation: `python run_evaluation.py --evaluate --provider your_provider`
3. Submit a PR with results

## ğŸ“„ License

MIT License â€” see [LICENSE](LICENSE) for details.

---

**Built by [Saketh Kesiraju](https://github.com/sakethkesiraju26)**
